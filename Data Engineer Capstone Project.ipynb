{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# U.S. Immigrants Analysis\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "I implemented the Udacity provided Data Engineering project using following datasets:\n",
    "* I94 Immigration Data: This data comes from the US National Tourism and Trade Office.  \n",
    "* U.S. City Demographic Data: This data comes from OpenSoft. \n",
    "* World Temperature Data: This dataset came from Kaggle. \n",
    "* Airport Code Data: This is a simple table of airport codes and corresponding cities. \n",
    "* I94_SAS_Labels_Descriptions.SAS - used the descriptions available in this file\n",
    "\n",
    "Used AWS EMR to preprocess the above datasets and store it into S3. Then load this data to AWS Redshift fact && dimension tables, applied  data quality checks using Apache Airflow data pipeline. The Redshift datawarehouse can be used for further analysis of the data using SQL. \n",
    "\n",
    "The project steps are:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "    - Since the scope of the project will be highly dependent on the data, these two things happen simultaneously. In this step, youâ€™ll:\n",
    "    Identify and gather the data you'll be using for your project (at least two sources and more than 1 million rows). \n",
    "    See Project Resources for ideas of what data you can use.\n",
    "    Explain what end use cases you'd like to prepare the data for (e.g., analytics table, app back-end, source-of-truth database, etc.)\n",
    "\n",
    "* Step 2: Explore and Assess the Data\n",
    "    - Explore the data to identify data quality issues, like missing values, duplicate data, etc.\n",
    "    Document steps necessary to clean the data\n",
    "\n",
    "* Step 3: Define the Data Model\n",
    "    - Map out the conceptual data model and explain why you chose that model\n",
    "    List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "* Step 4: Run ETL to Model the Data\n",
    "    - Create the data pipelines and the data model\n",
    "    Include a data dictionary\n",
    "    Run data quality checks to ensure the pipeline ran as expected\n",
    "    Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    "    Unit tests for the scripts to ensure they are doing the right thing\n",
    "    Source/count checks to ensure completeness\n",
    "\n",
    "* Step 5: Complete Project Write Up\n",
    "    - What's the goal? What queries will you want to run? How would Spark or Airflow be incorporated? Why did you choose the model you chose?\n",
    "    Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    Document the steps of the process.\n",
    "    Propose how often the data should be updated and why.\n",
    "    Post your write-up and final data model in a GitHub repo.\n",
    "    Include a description of how you would approach the problem differently under the following scenarios:\n",
    "    If the data was increased by 100x.\n",
    "    If the pipelines were run on a daily basis by 7am.\n",
    "    If the database needed to be accessed by 100+ people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "import psycopg2\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Deliver a cloud based data warehouse solution that supports integrations with BI tools. I chose AWS Redshift as the DWH.  \n",
    "\n",
    "Users of the DWH tool should be able to analyze the datamart and find answers such as:\n",
    "applicants by nationality/origin/airplane.\n",
    "Correlations between destination in the U.S and the applicant's country.\n",
    "Correlations between climates. \n",
    "Correlations between applicant's demographics, and states visited in U.S.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "- I94 Immigration Data: This data comes from the US National Tourism and Trade Office.\n",
    "- U.S. City Demographic Data: This data comes from OpenSoft.\n",
    "- World Temperature Data: This dataset came from Kaggle.\n",
    "- Airport Code Data: This is a simple table of airport codes and corresponding cities.\n",
    "- I94_SAS_Labels_Descriptions.SAS - used the descriptions available in this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "In the following sections I briefly describe the datasets provided and give a summarized idea on the reasons I took into consideration when deciding what data to use.\n",
    "\n",
    "#### U.S Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the U.S Immigration data\n",
    "us_immigraiton_fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "us_immigraiton_df = pd.read_sas(us_immigraiton_fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20130811</td>\n",
       "      <td>SEO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate  i94bir  i94visa  count  dtadfile visapost occup entdepa entdepd  \\\n",
       "0      NaN    37.0      2.0    1.0       NaN      NaN   NaN       T     NaN   \n",
       "1      NaN    25.0      3.0    1.0  20130811      SEO   NaN       G     NaN   \n",
       "2  20691.0    55.0      2.0    1.0  20160401      NaN   NaN       T       O   \n",
       "3  20567.0    28.0      2.0    1.0  20160401      NaN   NaN       O       O   \n",
       "4  20567.0     4.0      2.0    1.0  20160401      NaN   NaN       O       O   \n",
       "\n",
       "  entdepu matflag  biryear   dtaddto gender insnum airline        admnum  \\\n",
       "0       U     NaN   1979.0  10282016    NaN    NaN     NaN  1.897628e+09   \n",
       "1       Y     NaN   1991.0       D/S      M    NaN     NaN  3.736796e+09   \n",
       "2     NaN       M   1961.0  09302016      M    NaN      OS  6.666432e+08   \n",
       "3     NaN       M   1988.0  09302016    NaN    NaN      AA  9.246846e+10   \n",
       "4     NaN       M   2012.0  09302016    NaN    NaN      AA  9.246846e+10   \n",
       "\n",
       "   fltno visatype  \n",
       "0    NaN       B2  \n",
       "1  00296       F1  \n",
       "2     93       B2  \n",
       "3  00199       B2  \n",
       "4  00199       B2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_immigraiton_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### The immigration dataset is our fact table (star schema in the data warehouse)\n",
    "\n",
    "#### Data dictionary\n",
    "\n",
    "- cicid - ID that uniquely identify one record in the dataset\n",
    "- i94yr - 4 digit year\n",
    "- i94mon - Numeric month\n",
    "- i94cit - 3 digit code of source city for immigration (Born country)\n",
    "- i94res - 3 digit code of source country for immigration (Residence country)\n",
    "- i94port - Port addmitted through\n",
    "- arrdate - Arrival date in the USA\n",
    "- i94mode - Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)\n",
    "- i94addr - State of arrival\n",
    "- depdate - Departure date\n",
    "- i94bir - Age of Respondent in Years\n",
    "- i94visa - Visa codes collapsed into three categories: (1 = Business; 2 = Pleasure; 3 = Student)\n",
    "- count - Used for summary statistics\n",
    "- dtadfile - Character Date Field\n",
    "- visapost - Department of State where where Visa was issued\n",
    "- occup - Occupation that will be performed in U.S.\n",
    "- entdepa - Arrival Flag. Whether admitted or paroled into the US\n",
    "- entdepd - Departure Flag. Whether departed, lost visa, or deceased\n",
    "- entdepu - Update Flag. Update of visa, either apprehended, overstayed, or updated to PR\n",
    "- matflag - Match flag\n",
    "- biryear - 4 digit year of birth\n",
    "- dtaddto - Character date field to when admitted in the US\n",
    "- gender  - Gender\n",
    "- insnum  - INS number \n",
    "- airline - Airline used to arrive in U.S. \n",
    "- admnum  - Admission number, should be unique and not nullable \n",
    "- fltno   - Flight number of Airline used to arrive in U.S.\n",
    "- visatype - Class of admission legally admitting the non-immigrant to temporarily stay in U.S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Global Temperature data \n",
    "In this project, we will use the data available in ../../data2/GlobalLandTemperaturesByCity\n",
    "\n",
    "#### Data dictionary\n",
    "- dt - Date in format YYYY-MM-DD\n",
    "- AverageTemperature - Average temperature of the city in a given date\n",
    "- AverageTemperatureUncertainty - Average temperature of the city\n",
    "- City - City Name\n",
    "- Country - Country Name\n",
    "- Latitude - Latitude\n",
    "- Longitude - Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperature_fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "world_temperature_df = pd.read_csv(temperature_fname)\n",
    "\n",
    "# Group by country and produce average temperature levels per country\n",
    "world_temperature_df = world_temperature_df.groupby([\"Country\"]).agg({\n",
    "    \"AverageTemperature\": \"mean\", \n",
    "    \"Latitude\": \"last\", \n",
    "    \"Longitude\": \"last\"}\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>13.816497</td>\n",
       "      <td>36.17N</td>\n",
       "      <td>69.61E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>15.525828</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>19.17E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>17.763206</td>\n",
       "      <td>31.35N</td>\n",
       "      <td>5.65E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Angola</td>\n",
       "      <td>21.759716</td>\n",
       "      <td>15.27S</td>\n",
       "      <td>14.17E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>16.999216</td>\n",
       "      <td>26.52S</td>\n",
       "      <td>64.48W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country  AverageTemperature Latitude Longitude\n",
       "0  Afghanistan           13.816497   36.17N    69.61E\n",
       "1      Albania           15.525828   40.99N    19.17E\n",
       "2      Algeria           17.763206   31.35N     5.65E\n",
       "3       Angola           21.759716   15.27S    14.17E\n",
       "4    Argentina           16.999216   26.52S    64.48W"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_temperature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Airports data \n",
    "lets read the data available in airport-codes_csv.csv\n",
    "\n",
    "#### Data dictionary\n",
    " - ident - Unique identifier for the airport\n",
    " - type - Airport type\n",
    " - name - Airport Name\n",
    " - elevation_ft - Altitude of the airport\n",
    " - continent - Continent\n",
    " - iso_country - ISO code of airport country\n",
    " - iso_region - ISO code for airport region\n",
    " - municipality - City\n",
    " - gps_code - GPS code of airport\n",
    " - iata_code - IATA code of airport\n",
    " - local_code - Local code of airport\n",
    " - coordinates - GPS coordinates of airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airports_df = pd.read_csv(\"airport-codes_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### After exploring the airport data I realized it was not possible to join with the immigrants data due to the absence of common keys. \n",
    "So I decided to discard the airport data for the data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### U.S. City Demographic data \n",
    "This data comes from OpenSoft.It contains information about the demographics of the U.S. cities(means the census informaiton)\n",
    "lets read the data available in us-cities-demographics.csv\n",
    "\n",
    "#### Data dictionary \n",
    "- City\n",
    "- State\n",
    "- Median Age - Average age of the populaiton\n",
    "- Male Population\n",
    "- Female Population\n",
    "- Total Population\n",
    "- Number of Veterans\n",
    "- Foreign-born\n",
    "- Average Household Siz\n",
    "- State Code - Common key sharing with Immigrants dataset\n",
    "- Race\n",
    "- Count - Count in each Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "usa_cities_demographics_df = pd.read_csv(\"us-cities-demographics.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usa_cities_demographics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "I am using 3 data sources provided by the Udacity : 94 Immigration Data, World Temperature Data and U.S. City Demographic Data. Also,  used descriptions from file I94_SAS_Labels_Descriptions.SAS\n",
    "\n",
    "I am following the STAR schema concept.\n",
    "\n",
    "Tables in the Data model are:\n",
    "* 1 -IMMIGRATION - The U.S Immigration Dataset is the FACT table here\n",
    "* 2 - STATE - Dimension table created from U.S. City Demographic Data\n",
    "* 3 - COUNTRY - Dimension table created from World Temperature Data\n",
    "* 4 - DATE - Dimension table extracted from the imigraiton dataset\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "The actual ETL was done in a dedicated file **etl/s3_to_redshift_etl.py** using AWS EMR.\n",
    "But, lets explore the ETL funcitons in this notebook as well.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from etl.s3_to_redshift_etl import etl_immigration_data, etl_countries_data, etl_states_data, create_spark_session, save, read_data, cast_type, capitalize_udf, change_field_value_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Spark Session\n",
    "import os, re\n",
    "import configparser\n",
    "from datetime import timedelta, datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, when, lower, isnull, year, month, dayofmonth, hour, weekofyear, dayofweek, date_format, to_date\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, DoubleType\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ab61ce8d6627:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f35ef179a90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**ETL U.S Immigration dataset**\n",
    "\n",
    "Loading of the immigration file into Spark dataframe and Save the processed immigration and date dataframes to the Amazon S3 in the parquet format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration = etl_immigration_data(spark, input_path='../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat',\n",
    "                                     output_path=\"s3a://udemy-data-engineer-capstone/immigration.parquet\",\n",
    "                                     date_output_path=\"s3a://udemy-data-engineer-capstone/date.parquet\",\n",
    "                                     input_format = \"com.github.saurfang.sas.spark\", \n",
    "                                     load_size=1000, partitionBy=None, \n",
    "                                     columns_to_save = '*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Countries dataset**\n",
    "\n",
    "Loading of the global temperature dataset file into Spark dataframe and Save the country dataframe to the Amazon S3 in the parquet format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "countries = etl_countries_data(spark, output_path=\"s3a://udemy-data-engineer-capstone/country.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**States dataset**\n",
    "\n",
    "Loading of the U.S. cities demographics dataset file into Spark dataframe and join it with I94ADDR.csv then save the merged dataframe into S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "states = etl_states_data(spark, output_path=\"s3a://udemy-data-engineer-capstone/state.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here I used **Apache Airflow** to build a DAG to extract data from S3 and load it into Redshift staging tables of the same name in Amazon Redshift. \n",
    "As a final step I applied  the data quality(simply counts) checking to ensure completeness.\n",
    "\n",
    "Airflow data pipeline code is in the \"airflow\" folder.  \n",
    "\n",
    "**Important files:**\n",
    " - airflow/dags/dag_capstone_s3_to_redshift.py - The main DAG file to load data from S3 to Redshift\n",
    " - airflow/dags/plugins/stage_redshift.py - Custom operator that performs the s3 to redshift transfer\n",
    " - airflow/dags/plugins/data_quality.py - Custom operator that checks redshift data quality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "- All the tables have a PrimaryKey constraint that uniquely identifies the records\n",
    "- In the immigraiton fact table there are Foriegn Keys that guarantee that values in the fact table are present in the dimension tables(country, state).\n",
    "\n",
    "In the data quality check step, this check I verified if every redshift table was actually loaded with count check in all the tables of the data model.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "**Immigration** fact table\n",
    "- cicid\t\t- primary key\n",
    "- i94yr\t\t- year\n",
    "- i94mon\t- month\n",
    "- i94cit\t- 3 digit for the country code where the visitor was born. this is a fk to the country dimension table\n",
    "- i94res\t- 3 digit for the country code where the visitor resides in. this is a fk to the country dimension table\n",
    "- arrdate\t- arrival date in the usa. this is a fk to the date dimension table\n",
    "- i94mode\t- mode of transportation (1 = air; 2 = sea; 3 = land; 9 = not reported)\n",
    "- i94addr\t- state of arrival. this is a fk to the state dimension table\n",
    "- depdate\t- departure date from the usa. this is a fk to the date dimension table\n",
    "- i94bir\t- age of respondent in years\n",
    "- i94visa\t- visa codes collapsed into three categories: (1 = business; 2 = pleasure; 3 = student)\n",
    "- biryear\t- 4 digit year of birth\n",
    "- gender\t- gender\n",
    "- airline\t- airline used to arrive in u.s.\n",
    "- fltno\t\t- flight number of airline used to arrive in u.s.\n",
    "- visatype\t- class of admission legally admitting the non-immigrant to temporarily stay in u.s.\n",
    "- stay\t\t- number of days in the us\n",
    "\n",
    "**Country** dimension table\n",
    "- code\t\t\t- country code. this is the pk.\n",
    "- country\t\t- country name\n",
    "- temperature\t- average temperature of the country between 1743 and 2013\n",
    "- latitude\t\t- gps latitude\n",
    "- longitude\t\t- gps longitude\n",
    "\n",
    "**State** dimension table\n",
    "- code\t\t\t\t\t\t- primary key. this is the code of the state as in i94addr lookup table\n",
    "- state\t\t\t\t\t\t- name of the state\n",
    "- blackorafricanamerican\t- number of residents of the race black or african american\n",
    "- white\t\t\t\t\t\t- number of residents of the race white\n",
    "- foreignborn\t\t\t\t- number of residents that born outside th united states\n",
    "- americanindianandalaskanative\t- number of residents of the race american indian and alaska native\n",
    "- hispanicorlatino\t- number of residents of the race hispanic or latino\n",
    "- asian\t- number of residents of the race asian\n",
    "- numberveterans\t- number of residents that are war veterans\n",
    "- femalepopulation\t- number of female population\n",
    "- malepopulation\t- number of male population\n",
    "- totalpopulation\t- number total of the population\n",
    "\n",
    "**Date** dimension table\n",
    "- date - Date in the format YYYY-MM-DD. This is the PK.\n",
    "- day - Two digit day\n",
    "- month - Two digit month\n",
    "- year - Four digit for the year\n",
    "- weekofyea - The week of the year\n",
    "- dayofweek - The day of the week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    - Python - this project is written in python\n",
    "    - Pandas - Used for initial data analysis\n",
    "    - Apark Spark (AWS EMR) - Used pyspark to pre-process the data and store it into S3\n",
    "    - AWS S3 - used as intermediate data store for pre-processed data from Spark\n",
    "    - AWS Redshift - Final datamark which stores the processed and cleansed data in the form of fact and dimension tables. Ready to be consumed by analysis using user's favorite BI tools \n",
    "    \n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "    - Monthly basis is a good start\n",
    "    \n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    "\n",
    " * The data was increased by 100x.\n",
    "     - This project is built using EMR, S3 an Redshift. EMR can handle any large amount of data, so dies S3 and so does Redshift\n",
    "     - Few more improvesments could be to enable the better compression mechanism in S3 and also enable parallel processing(copy) in Redshift\n",
    "     \n",
    "     \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     - This is easily doable on the Airflow DAG\n",
    "     \n",
    " \n",
    " * The database needed to be accessed by 100+ people.\n",
    "     - Redshift can support upto 500 connections. Also elastic resize option in Redshift comes handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
